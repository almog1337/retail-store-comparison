# Scraper Project

## Installation

This project uses [uv](https://docs.astral.sh/uv/) for dependency management.

To install the required dependencies, run:

```bash
uv sync
```

This will install all packages specified in `pyproject.toml` into a virtual environment.

## Configuration

The scraper requires environment variables to connect to the uploader service:

```bash
export UPLOADER_URL="http://localhost:8000/minio"
export UPLOADER_API_KEY="dev-key"
```

## Usage

After installation, you can run the scraper pipeline using:

```bash
python main.py
```

## Data Flow

1. **Extract:** Scraper downloads and parses data files from retail chains
2. **Group:** Records are grouped by `SubChainId`, `StoreId`, and `BikoretNo`
3. **Upload:** Grouped records are sent to the uploader service
4. **Store:** Uploader validates records and auto-generates storage keys with Hive-style partitioning

**Note:** Storage keys are now automatically generated by the uploader service based on record metadata and pipeline name. The scraper no longer handles key generation.

## Project Structure

- **abstractions/**: Core interfaces and base classes for the scraping pipeline
- **shufersal/**: Shufersal-specific scraper implementation
- **uploaders/**: Utilities for uploading scraped data
- **parsers/**: Data parsing modules
- **fetchers/**: Data fetching modules
- **configs/**: Configuration files

## Requirements

- Python 3.8+
- uv package manager
- Access to uploader service (see `../uploader/README.md`)
